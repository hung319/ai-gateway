import os
from typing import List, Optional
from sqlmodel import Session, select
from litellm import Router
from app.models import Provider
from app.config import REDIS_URL, ENABLE_CACHE, LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY

class AIEngine:
    def __init__(self):
        self.router: Optional[Router] = None
        
    def initialize(self, session: Session):
        """Kh·ªüi t·∫°o Router t·ª´ Database"""
        print("üîÑ [Engine] Initializing AI Router...")
        
        # 1. Setup Observability (Langfuse)
        if LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY:
            os.environ["LANGFUSE_PUBLIC_KEY"] = LANGFUSE_PUBLIC_KEY
            os.environ["LANGFUSE_SECRET_KEY"] = LANGFUSE_SECRET_KEY
            # LiteLLM t·ª± ƒë·ªông nh·∫≠n di·ªán qua bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ k√≠ch ho·∫°t callback
            from litellm import success_callback, failure_callback
            if "langfuse" not in success_callback: success_callback.append("langfuse")
            if "langfuse" not in failure_callback: failure_callback.append("langfuse")
            print("‚úÖ [Engine] Langfuse Enabled")

        # 2. Load Providers t·ª´ DB
        providers = session.exec(select(Provider)).all()
        model_list = []
        
        for p in providers:
            # C·∫•u h√¨nh t·ª´ng deployment cho Router
            # LiteLLM Router c·∫ßn format: [{ "model_name": "gpt-4", "litellm_params": { ... } }]
            
            # Ta s·∫Ω map t·∫•t c·∫£ provider v·ªÅ m·ªôt model "·∫£o" ho·∫∑c gi·ªØ nguy√™n t√™n model
            # ƒê·ªÉ ƒë∆°n gi·∫£n h√≥a Fallback, ta c·∫ßn user c·∫•u h√¨nh nhi·ªÅu provider c√πng lo·∫°i.
            # V√≠ d·ª•: Provider A (OpenAI), Provider B (Azure) ƒë·ªÅu ph·ª•c v·ª• model "gpt-4o"
            
            deployment = {
                "model_name": p.name, # Alias d√πng ƒë·ªÉ routing (vd: gpt-4o)
                "litellm_params": {
                    "model": f"{p.provider_type}/{p.name}" if p.provider_type != "openai" else p.name,
                    "api_key": p.api_key,
                }
            }
            
            if p.base_url:
                deployment["litellm_params"]["api_base"] = p.base_url
                
            model_list.append(deployment)

        # 3. Init Router v·ªõi Redis Cache
        router_kwargs = {
            "model_list": model_list,
            # C·∫•u h√¨nh Fallback: N·∫øu 1 provider l·ªói, th·ª≠ c√°i ti·∫øp theo trong list c√πng t√™n model
            "fallbacks": [], 
            "set_verbose": False
        }
        
        if REDIS_URL and ENABLE_CACHE:
            router_kwargs["redis_host"] = os.getenv("REDIS_HOST", "redis")
            router_kwargs["redis_port"] = int(os.getenv("REDIS_PORT", 6379))
            router_kwargs["redis_password"] = os.getenv("REDIS_PASSWORD", None)
            router_kwargs["cache_responses"] = True
            print(f"‚úÖ [Engine] Caching Enabled (Redis)")

        self.router = Router(**router_kwargs)
        print(f"üöÄ [Engine] Router Ready with {len(model_list)} providers")

    async def reload(self, session: Session):
        """Reload n√≥ng khi Admin thay ƒë·ªïi c·∫•u h√¨nh"""
        self.initialize(session)

# Global Instance
ai_engine = AIEngine()
